{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Data Time-Series\n",
    "\n",
    "You are again moving to another role, not at *The Weather Channel*, where you are ask to create a Weather Forecasting Model.\n",
    "\n",
    "For that, you will be using *Jena Climate* dataset recorded by the *Max Planck Institute for Biogeochemistry*. \n",
    "\n",
    "The dataset consists of 14 features such as temperature, pressure, humidity etc, recorded **once per 10 minutes**.\n",
    "\n",
    "Location: Weather Station, Max Planck Institute for Biogeochemistry in Jena, Germany\n",
    "\n",
    "Time-frame Considered: **Jan 10, 2009 - December 31, 2012**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load your data\n",
    "Your data can be found on the Deep Learning Module under a file named: climate_data_2009_2012.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"climate_data_2009_2012.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Data engineering\n",
    "\n",
    "You are given 3 lists:\n",
    "- titles: Display names of your columns\n",
    "- feature_keys: Names of the columns used as features\n",
    "- colors: The color to use when ploting that column's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    \"Pressure\",\n",
    "    \"Temperature\",\n",
    "    \"Temperature in Kelvin\",\n",
    "    \"Temperature (dew point)\",\n",
    "    \"Relative Humidity\",\n",
    "    \"Saturation vapor pressure\",\n",
    "    \"Vapor pressure\",\n",
    "    \"Vapor pressure deficit\",\n",
    "    \"Specific humidity\",\n",
    "    \"Water vapor concentration\",\n",
    "    \"Airtight\",\n",
    "    \"Wind speed\",\n",
    "    \"Maximum wind speed\",\n",
    "    \"Wind direction in degrees\",\n",
    "]\n",
    "\n",
    "feature_keys = [\n",
    "    \"p (mbar)\",\n",
    "    \"T (degC)\",\n",
    "    \"Tpot (K)\",\n",
    "    \"Tdew (degC)\",\n",
    "    \"rh (%)\",\n",
    "    \"VPmax (mbar)\",\n",
    "    \"VPact (mbar)\",\n",
    "    \"VPdef (mbar)\",\n",
    "    \"sh (g/kg)\",\n",
    "    \"H2OC (mmol/mol)\",\n",
    "    \"rho (g/m**3)\",\n",
    "    \"wv (m/s)\",\n",
    "    \"max. wv (m/s)\",\n",
    "    \"wd (deg)\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    \"blue\",\n",
    "    \"orange\",\n",
    "    \"green\",\n",
    "    \"red\",\n",
    "    \"purple\",\n",
    "    \"brown\",\n",
    "    \"pink\",\n",
    "    \"gray\",\n",
    "    \"olive\",\n",
    "    \"cyan\",\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the climate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to show a plot of each column (using the respective color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_raw_visualization(data, date_time_key):\n",
    "    time_data = data[date_time_key]\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    for i in range(len(feature_keys)):\n",
    "        key = feature_keys[i]\n",
    "        c = colors[i % (len(colors))]\n",
    "        t_data = data[key]\n",
    "        t_data.index = time_data\n",
    "        t_data.head()\n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i // 2, i % 2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display each column in a plot using above funciton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_raw_visualization(df, \"Date Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we have lots of data, this can be a challenge when we train our model, to resolve that we will reduce the resolution of our data, instead of having a climate signal each 10 minutes, we will have it each hour\n",
    "\n",
    "- Add a new column to your dataframe with the Date Time information\n",
    "- Name that column FormatedDateTime\n",
    "- Convert that column into date time data type\n",
    "- Set that column as the dataframe index\n",
    "- Regroup data to be each 1 hour instead of each 10 minutes\n",
    "- Save the grouped data into a dataframe called df_resampled\n",
    "- Remove the FormatedDateTime as the index.\n",
    "- Show the top 5 rows of df_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FormatedDateTime'] = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
    "df = df.set_index('FormatedDateTime')\n",
    "df_resampled = df[feature_keys].resample('H').mean()\n",
    "df_resampled = df_resampled.reset_index()\n",
    "\n",
    "df_resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our fields again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_raw_visualization(df_resampled, \"FormatedDateTime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Data Split: Train and Evaluation datasets.\n",
    "\n",
    "- We are tracking data from past 120 timestamps (120 hours = 5 days). \n",
    "- This data will be used to predict the temperature after 12 timestamps (12 hours).\n",
    "- Since every feature has values with varying ranges, we do normalization to confine feature values to a range of [0, 1] before training a neural network. \n",
    "- We do this by subtracting the mean and dividing by the standard deviation of each feature in the *normalize* function\n",
    "- The model is shown data for first 5 days i.e. 120 observations, that are sampled every hour. \n",
    "- The temperature after 12 hours observation will be used as a label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% of the data will be used for training, the rest for testing\n",
    "split_fraction = 0.7\n",
    "# The number of samples is the number of rows in the data\n",
    "number_of_samples = df_resampled.shape[0]\n",
    "# The size in rows of the split dataset\n",
    "train_split = int(split_fraction * int(number_of_samples))\n",
    "\n",
    "# Number of samples in the past used to predict the future\n",
    "past = 120\n",
    "# Number of samples in the future to predict (the value in the 72nd hour is our label)\n",
    "future = 12\n",
    "# Learning rate parameter for the Adam optimizer\n",
    "learning_rate = 0.001\n",
    "# Batch size for the model training\n",
    "batch_size = 256\n",
    "# Number of epochs for the model training\n",
    "epochs = 10\n",
    "\n",
    "# Another way to normalize the data (all columns in the same range)\n",
    "def normalize(data, train_split):\n",
    "    data_mean = data[:train_split].mean(axis=0)\n",
    "    data_std = data[:train_split].std(axis=0)\n",
    "    return (data - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's select the following parameters as our features: \n",
    "   - Pressure, Temperature, Saturation vapor pressure, Vapor pressure deficit, Specific humidity, Airtight, Wind speed\n",
    "\n",
    "- Set the column FormatedDateTime as the index of our dataframe.\n",
    "   - This is important since now, FormatedDateTime is used as our datetime field and not as a Feature field\n",
    "\n",
    "- Normalize all fields\n",
    "\n",
    "- Generate two datasets:\n",
    "   - train_data: Train dataset with our normalized fields\n",
    "   - val_data: Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The selected parameters are:\",\n",
    "    \", \".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),\n",
    ")\n",
    "selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]\n",
    "features = df_resampled[selected_features]\n",
    "features.index = df_resampled[\"FormatedDateTime\"]\n",
    "print(features.head())\n",
    "\n",
    "features = normalize(features.values, train_split)\n",
    "features = pd.DataFrame(features)\n",
    "print(features.head())\n",
    "\n",
    "train_data = features.loc[0 : train_split - 1]\n",
    "val_data = features.loc[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here we need to set our Label Dataset.\n",
    "\n",
    "- We want to use the last 5 days of data, to predict the next 12 hours\n",
    "- This means that our label starts at the 12th hour after the history data.\n",
    "   - [.......................... **.** ]\n",
    "   - -----Start----->\n",
    "- And it will end at the end of our train dataset size.\n",
    "   - <-------- Train ---------> <--- Test --->\n",
    "   - [.................................................**|**...........]\n",
    "   - ----------End---------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = past + future\n",
    "end = start + train_split\n",
    "\n",
    "x_train = train_data[[i for i in range(7)]].values\n",
    "y_train = features.iloc[start:end][[1]]\n",
    "\n",
    "step = 1\n",
    "sequence_length = past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *timeseries_dataset_from_array* function takes in a sequence of data-points gathered at equal intervals, along with time series parameters such as length of the sequences/windows, spacing between two sequence/windows, etc., to produce batches of sub-timeseries inputs and targets sampled from the main timeseries.\n",
    "\n",
    "- Input data (hour features) = x_train\n",
    "- The **corresponding** value of the temperature 12 hours into the future = y_train\n",
    "- Since we want to use 5 days of data to predict the future temperature then: sequence_length = 120\n",
    "- Since we want to sample every hour then: sampling_rate = 1\n",
    "- Let's use a common batch size of 256 (variable above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare our validation dataset:\n",
    "\n",
    "- The validation dataset must not contain the last 120+12 rows as we won't have label data for those records, hence these rows must be subtracted from the end of the data.\n",
    "- The validation label dataset must start from 120+12 after train_split, hence we must add past + future to label_start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_end = len(val_data) - past - future\n",
    "\n",
    "label_start = train_split + past + future\n",
    "\n",
    "x_val = val_data.iloc[:x_end][[i for i in range(7)]].values\n",
    "y_val = features.iloc[label_start:][[1]]\n",
    "\n",
    "dataset_val = keras.preprocessing.timeseries_dataset_from_array(\n",
    "    x_val,\n",
    "    y_val,\n",
    "    sequence_length=sequence_length,\n",
    "    sampling_rate=step,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "for batch in dataset_train.take(1):\n",
    "    inputs, targets = batch\n",
    "\n",
    "print(\"Input shape:\", inputs.numpy().shape)\n",
    "print(\"Target shape:\", targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Define and Compile your model:\n",
    "- An input layer \n",
    "- A Long Short-Term Memory Hidden Layer with 32 units. LSTM is a type of recurrent neural network layer that is well-suited for time series data.\n",
    "- An output Dense Layer (Linear Activation function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\n",
    "lstm_out = keras.layers.LSTM(32)(inputs)\n",
    "outputs = keras.layers.Dense(1)(lstm_out)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Train your model:\n",
    "\n",
    "Specify the file path where the model's weights will be saved with:\n",
    "`path_checkpoint = \"model_checkpoint.weights.h5\"`\n",
    "\n",
    "We want to add a callback to stop training when a monitored metric stops improving:\n",
    "`es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)`\n",
    "\n",
    "Train the model using Fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = \"model_checkpoint.weights.h5\"\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n",
    "\n",
    "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    filepath=path_checkpoint,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_train,\n",
    "    epochs=epochs,\n",
    "    validation_data=dataset_val,\n",
    "    callbacks=[es_callback, modelckpt_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results of your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_loss(history, \"Training and Validation Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make 5 predictions and display the predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n",
    "    marker = [\".-\", \"rx\", \"go\"]\n",
    "    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "\n",
    "    plt.title(title)\n",
    "    for i, val in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future + 5) * 2])\n",
    "    plt.xlabel(\"Time-Step\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "for x, y in dataset_val.take(5):\n",
    "    show_plot(\n",
    "        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],\n",
    "        12,\n",
    "        \"Single Step Prediction\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a Time Series Forecasting where using the last 3 days you will predict the weather in the next 3 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
